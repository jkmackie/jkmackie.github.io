---
title: "About Me"
date: 2023-01-22T23:25:30-06:00
type: "page"
---

![Justin](./jmack_image.jpeg)

Hi my name is Justin.  :wave:  I am a consultant with a passion for machine learning, analytics, research, and coding.

Here are my [**LinkedIn**](https://www.linkedin.com/in/justinmackie/) and [**GitHub**](https://github.com/jkmackie) profiles.

---
## **Education and Certifications:**

* [**Texas A&M University**](https://www.tamu.edu/) - Finance MBA
* [**Trinity University**](https://www.trinity.edu/) - BS, Electrical Engineering; Minor in Mathematics

---

#### [**Microsoft Training Module Transcript**](https://learn.microsoft.com/en-us/users/justinmackie-4883/transcript/dzqm1il0l98pq84) - Azure and DataBricks

---

#### **TinyML - Edge device sensor data analytics:**
* [TinyML 1: Fundamentals of TinyML](https://courses.edx.org/certificates/5b30777345d246b4bed0b49894959449)
* [TinyML 2: Application of TinyML](https://courses.edx.org/certificates/773fb90c20584ebaa1fa17907557738b)
* [TinyML 3: Deploying TinyML](https://courses.edx.org/certificates/a3e0fbd64f754e879d00fa0d2f50570b)

---

#### **Natural Language Processing with Attention Models:**
* [Credential (Includes the famous Vaswani Transformer architecture with attention mechanism.)](https://coursera.org/verify/MXLWNU8HC4F5)
* Fun fact: ChatGPT uses **G**enerative **P**re-trained **T**ransformers.  The transformer architecture core concepts are: attention, self-attention, and positional encoding.
* **Attention** looks at all input words for output translation decisions.  **Self-attention** understands words in context of the words around it (meaning).  **Positional encoding** encodes the position of a word in a sequence.

---

#### [**DataQuest Path: Data Scientist in Python**](https://app.dataquest.io/verify_cert/MPHNSGR1FJL7T96EUDFX/)